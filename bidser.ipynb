{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645eca92",
   "metadata": {},
   "source": [
    "https://openneuro.org/datasets/ds000017/versions/00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a018ebf6-acff-4c38-a03e-d6708ea7aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.dcm2nii import Dcm2niix\n",
    "from nipype.interfaces import fsl\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import pydicom\n",
    "import subprocess\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ca67ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(errors,text):\n",
    "    errors.append(text)\n",
    "\n",
    "def get_protocol_name(image_path):\n",
    "    return pydicom.dcmread(image_path).ProtocolName\n",
    "\n",
    "def get_image_in_dir(directory):\n",
    "    dicom_list = os.listdir(directory)\n",
    "    return os.path.join(directory,dicom_list[0])\n",
    "\n",
    "def add_to_json(json_file,field,value):\n",
    "    while not os.path.exists(json_file):\n",
    "        time.sleep(0.00000001)\n",
    "\n",
    "    line = '{\\\\\\\\n\\\\t' + '\\\\\"' + field + '\\\\\"' + ':\\\\\"' + value + '\\\\\",' \n",
    "    with open(json_file, \"r+\") as f:\n",
    "        old = f.read()\n",
    "        f.seek(0) \n",
    "        f.write(line + old[1:len(old)])\n",
    "\n",
    "def write_json(data,output_dir,file_name):\n",
    "    json_object = json.dumps(data,indent=4)\n",
    "    outputJson = os.path.join(output_dir,file_name)\n",
    "    with open(outputJson, \"w+\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "def dicom_to_nifti(inputPath,outputPath,outputFile,singleFile=True,compression_level=1,z_flag='3'):\n",
    "    if use_nipype:\n",
    "        converter = Dcm2niix()\n",
    "        converter.inputs.source_dir = inputPath\n",
    "        converter.inputs.output_dir = outputPath\n",
    "        converter.inputs.out_filename = outputFile\n",
    "        converter.inputs.single_file = False\n",
    "        #converter.inputs.compression = compression_level\n",
    "        converter.inputs.compress = compress\n",
    "        return converter.run()\n",
    "    else:\n",
    "        subprocess.run([\"dcm2niix.exe\", \"-z\", z_flag, \"-o\", outputPath, \"-f\", outputFile, inputPath])\n",
    "\n",
    "def deface_image(inputPath):\n",
    "    subprocess.run([\"pydeface\", \"--outfile\", outputPath, \"--force\", inputPath], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True)\n",
    "\n",
    "def generate_task_jsons(task_settings,output_dir):\n",
    "    ## generate json for the tasks\n",
    "    for task_name in task_settings['task_names_full']:\n",
    "        if task_name not in task_settings['ignore_tasks']:\n",
    "            json_dict = {\n",
    "                \"RepetitionTime\": 2,\n",
    "                \"TaskName\": task_name,\n",
    "                \"Manufacturer\": \"Siemens\",\n",
    "                \"ManufacturersModelName\": \"MAGNETOM Tim Trio\",\n",
    "                \"MagneticFieldStrength\": 3\n",
    "            }\n",
    "            json_object = json.dumps(json_dict, indent = 4)\n",
    "            task = \"task-\" + task_name + '_bold.json'\n",
    "            outputJson = os.path.join(output_dir,task)\n",
    "            with open(outputJson, \"w+\") as outfile:\n",
    "                outfile.write(json_object)\n",
    "\n",
    "def check_progress(subs,progress):\n",
    "    f = os.path.join(progress_file_name,)\n",
    "    subjects_path = os.path.join(os.getcwd(),output_dir) # where to find subjects?\n",
    "\n",
    "    if os.path.exists(progress_file_name):\n",
    "        with open(f, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "        \n",
    "        missing_subjects = []\n",
    "        for subject in progress.keys():\n",
    "            subj_dir = os.path.join(subjects_path,subject)\n",
    "            if not os.path.exists(subj_dir):\n",
    "                print(subject,\": Processed data no longer exists. Participant will be re-processed.\")\n",
    "                missing_subjects.append(subject)\n",
    "        \n",
    "        subs = [x for x in subs if x not in progress.keys() or x in missing_subjects]\n",
    "\n",
    "        for subject in subs:\n",
    "            subj_dir = os.path.join(subjects_path,subject)\n",
    "            # if their progress was interrupted, delete their entire folder and start over.\n",
    "            if os.path.exists(subj_dir) and subject not in progress.keys():\n",
    "                print(subject,\": Process was interrupted. Participant will be re-processed.\")\n",
    "                shutil.rmtree(subj_dir)\n",
    "            # if their file doesn't exist, re-process them.\n",
    "\n",
    "    print(\"Previous conversion detected: \", list(progress.keys()), \"will not be processed.\")\n",
    "    return subs,progress\n",
    "\n",
    "def process_subjects(subs):\n",
    "    \n",
    "    errors = []\n",
    "\n",
    "    for subject in subs:\n",
    "        print(\"processing participant:\", subject)\n",
    "        \n",
    "        ######### check if subject run exists\n",
    "        if subject not in subject_runs:\n",
    "            err = subject + \": \" + \"does not have a specific subject run assigned! Check the subject_runs dictionary!\"\n",
    "            log_error(errors,err)\n",
    "            print(err)\n",
    "            continue\n",
    "\n",
    "        subject_run = subject_runs[subject]\n",
    "\n",
    "        if subject_run not in runs:\n",
    "            err = subject + \": \" + \"run type \" + subject_run + \" was not found in the runs dictionary\"\n",
    "            log_error(errors,err)\n",
    "            print(err)\n",
    "            continue\n",
    "\n",
    "        subject_run_type = runs[subject_run]\n",
    "\n",
    "        subject_dir = os.path.join(input_dir, subject)\n",
    "        sessionDirectory = 'ses-1'\n",
    "\n",
    "        ############ process the T1's\n",
    "        anatomical_images = [name for name in glob.glob(subject_dir+'/*T1_MPR*')]\n",
    "        anatomical_image_out = 'anat' # output directory name\n",
    "        anatomical_outputPath = os.path.join(os.getcwd(),output_dir,subject,sessionDirectory,anatomical_image_out)\n",
    "\n",
    "        if not os.path.exists(anatomical_outputPath):\n",
    "            os.makedirs(anatomical_outputPath)\n",
    "\n",
    "        for anat_image in anatomical_images:\n",
    "            inputPath = os.path.join(anat_image)\n",
    "            outputFile = subject + \"_\" + sessionDirectory + '_run-' + str(anatomical_images.index(anat_image)+1) + \"_T1w\"\n",
    "            dicom_to_nifti(inputPath,anatomical_outputPath,outputFile,z_flag)\n",
    "            if deface_anatomical:\n",
    "                deface_image(inputPath)\n",
    "\n",
    "        ##### process field maps\n",
    "        if process_field_maps:      \n",
    "            field_maps = [name for name in glob.glob(subject_dir+'/*FIELD*')]\n",
    "            field_maps = [field_maps[x:x+2] for x in range(0, len(field_maps), 2)]\n",
    "\n",
    "            fieldmap_out = 'fmap' # output directory name\n",
    "            fieldmap_outputpath = os.path.join(output_dir,subject,sessionDirectory,fieldmap_out)\n",
    "            if not os.path.exists(fieldmap_outputpath):\n",
    "                os.makedirs(fieldmap_outputpath)\n",
    "\n",
    "            # convert every fieldmap\n",
    "            for i in range(len(field_maps)):\n",
    "                for f_file in field_maps[i]:\n",
    "                    inputPath = os.path.join(f_file)\n",
    "                    outputFile = subject + \"_\" + sessionDirectory + '_run-' + str(i+1)\n",
    "                    dicom_to_nifti(inputPath,fieldmap_outputpath,outputFile,z_flag)\n",
    "\n",
    "            for name in field_map_names:\n",
    "                globber = '/*' + name + '*'\n",
    "                files = [name for name in glob.glob(fieldmap_outputpath+globber)]\n",
    "                for file_name in files:\n",
    "                    change_to = field_map_names[name]\n",
    "                    new_name = file_name.replace(name,change_to)\n",
    "                    os.rename(file_name, new_name)\n",
    "\n",
    "        ##### process functional images\n",
    "        functional_images_dir = [name for name in glob.glob(subject_dir+'/MOCOSERIES*')]\n",
    "        functional_images = [name for name in os.listdir(subject_dir) if name.find('MOCOSERIES') != -1]\n",
    "        functional_image_out = \"func\"\n",
    "        outputPath = os.path.join(output_dir,subject,sessionDirectory,functional_image_out)\n",
    "\n",
    "        if not os.path.exists(outputPath):\n",
    "            os.makedirs(outputPath)\n",
    "\n",
    "        for func_image in functional_images:\n",
    "            if func_image in subject_run_type:\n",
    "                taskName = subject_run_type[func_image][0]\n",
    "\n",
    "                if taskName not in task_settings['ignore_tasks']:\n",
    "                    run_anat = subject_run_type[func_image][2] # the anatomical image corresponding to this run\n",
    "                    anat_index = anat_map[run_anat]\n",
    "                    anat_file = anatomical_images[anat_index]\n",
    "\n",
    "                    inputPath = os.path.join(subject_dir,func_image)\n",
    "                    #print(get_protocol_name(get_image_in_dir(inputPath)))\n",
    "                    \n",
    "                    if taskName in task_settings['task_names_cond']:\n",
    "                        block_name = subject_blocks[subject]\n",
    "                        taskName = taskName + block_name\n",
    "\n",
    "                    outputFile = subject + \"_\" + sessionDirectory + \"_task-\" + taskName + \"_run-\" + subject_run_type[func_image][1] + \"_bold\"\n",
    "                    dicom_to_nifti(inputPath,outputPath,outputFile,z_flag)\n",
    "\n",
    "                    # if the 2nd anatomical image is paired with any functional image\n",
    "                    if anat_index != 0:\n",
    "                        err = subject + \": Anatomical image \" + anat_file[anat_file.find(\"T1\"):] + \" will be used for \" + func_image\n",
    "                        log_error(errors,err)\n",
    "        \n",
    "        # if there were MOCOSERIES folders in the directory which did not exist in the run dictionary, or visa-versa.\n",
    "        image_diff = set(list(subject_run_type.keys())).symmetric_difference(set(functional_images))\n",
    "        if len(image_diff) > 0:\n",
    "            err = subject + \": \" + str(image_diff) + \" was not paired with a corresponding MOCOSERIES in the directory or in the run dictionary.\"\n",
    "            log_error(errors,err)\n",
    "\n",
    "        if auto_detect_progress:\n",
    "            progress[subject] = 'done'\n",
    "            write_json(progress,progress_json_dir,progress_file_name)\n",
    "    \n",
    "    print(\"****************** CONVERSION COMPLETED ******************\")\n",
    "    print(\"**********************************************************\")\n",
    "    print(\"**********************************************************\")\n",
    "    print(\"Errors and notes regarding the BIDS conversion process will be printed below.\")\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "\n",
    "anat_map = {'first_anat': 0, \n",
    "            'second_anat': 1} # anatomical images are mapped to these indexes for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b1e5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_nipype = False # if you are able to use nipype, set this to true (for now, don't use nipype)\n",
    "deface_anatomical = False\n",
    "auto_detect_progress = True\n",
    "process_field_maps = True\n",
    "z_flag = \"3\" # corresponds to -z flag of dcm2niix. \"3\" will output 3D NIfTI files., \"n\" will output to 4D NIfTI files.\n",
    "input_dir = 'raw_data'\n",
    "output_dir = 'bids_3D'\n",
    "progress_json_dir = '.'\n",
    "progress_file_name = output_dir + '_progress' + '.json'\n",
    "\n",
    "subs = [name for name in os.listdir(input_dir) if os.path.join(os.getcwd(), input_dir,name) and name.find('sub') != -1] # all subject directories\n",
    "exclude_subjects = [] # exclude subjects from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3916397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your subject complies to the regular way of you collecting data, which means all sessions are in the same order and exactly same, you use this layout.\n",
    "# #block type (condition) session name,  task_name, run no, anatomical image to use\n",
    "runs = {\n",
    "    'A': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0012':('thinkrest','01','first_anat')},\n",
    "    'B': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0012':('thinkrest','01','first_anat')},\n",
    "    'C': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('discr','04','first_anat'),\n",
    "                    'MOCOSERIES_0012':('easydiff','01','first_anat')},\n",
    "    'no_easydiff': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('thinkrest','01','first_anat')},\n",
    "    'no_thinkrest': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('easydiff','01','first_anat')},\n",
    "    'series_8': {'MOCOSERIES_0008':('discr','01','first_anat')}, # A block OBL\n",
    "    'series_9': {'MOCOSERIES_0009':('discr','01','first_anat')}, # A block OBL\n",
    "    'series_10': {'MOCOSERIES_0010':('discr','01','first_anat')}, # A block OBL\n",
    "    'sub-19': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0012':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0014':('thinkrest','01','first_anat')},\n",
    "    'sub-06': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat')},\n",
    "    'sub-08': {'MOCOSERIES_0008':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0010':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0012':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0014':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0016':('thinkrest','01','first_anat')},\n",
    "    'sub-09': {'MOCOSERIES_0006':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0010':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0012':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0014':('thinkrest','01','first_anat')},\n",
    "    'sub-11': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0010':('easydiff','01','first_anat'),\n",
    "                    'MOCOSERIES_0014':('thinkrest','01','first_anat')},\n",
    "    'sub-18': {'MOCOSERIES_0006':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0010':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0012':('discr','04','first_anat'),\n",
    "                    'MOCOSERIES_0014':('easydiff','01','first_anat')},\n",
    "    'sub-31': {'MOCOSERIES_0006':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0008':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0012':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0014':('easydiff','01','first_anat')},\n",
    "    'sub-32': {'MOCOSERIES_0004':('discr','01','first_anat'),\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0010':('discr','03','first_anat')},\n",
    "    'sub-34': {'MOCOSERIES_0006':('discr','01','first_anat'), # CEGU\n",
    "                    'MOCOSERIES_0008':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0012':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0015':('easydiff','01','first_anat')},\n",
    "    'sub-37': {'MOCOSERIES_0004':('discr','01','first_anat'), # CEGU\n",
    "                    'MOCOSERIES_0006':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0012':('discr','03','first_anat')},\n",
    "    'sub-41': {'MOCOSERIES_0006':('discr','01','first_anat'), # CEGU\n",
    "                    'MOCOSERIES_0011':('discr','02','first_anat'),\n",
    "                    'MOCOSERIES_0019':('discr','03','first_anat'),\n",
    "                    'MOCOSERIES_0021':('easydiff','01','first_anat')}\n",
    "}\n",
    "\n",
    "field_map_names = {'e1_1.nii': 'magnitude1.nii',\n",
    "            'e1.json': 'magnitude1.json',\n",
    "            'e2_1.nii': 'magnitude2.nii',\n",
    "            'e2.json': 'magnitude2.json',\n",
    "            'e2_ph_1.nii': 'phasediff.nii',\n",
    "            'e2_ph.json': 'phasediff.json'}\n",
    "\n",
    "# map each subject to their specific run type, which is defined above\n",
    "subject_runs = {\n",
    "    'sub-01': 'no_thinkrest',\n",
    "    'sub-02': 'no_thinkrest',\n",
    "    'sub-03': 'no_thinkrest',\n",
    "    'sub-04': 'A',\n",
    "    'sub-05': 'B',\n",
    "    'sub-06': 'sub-06',\n",
    "    'sub-07': 'no_thinkrest',\n",
    "    'sub-08': 'sub-08',\n",
    "    'sub-09': 'sub-09',\n",
    "    'sub-10': 'C',\n",
    "    'sub-11': 'sub-11',\n",
    "    'sub-12': 'B',\n",
    "    'sub-13': 'B',\n",
    "    'sub-14': 'no_easydiff',\n",
    "    'sub-15': 'C',\n",
    "    'sub-16': 'C',\n",
    "    'sub-17': 'C',\n",
    "    'sub-18': 'sub-18',\n",
    "    'sub-19': 'sub-19',\n",
    "    'sub-20': 'A',\n",
    "    'sub-21': 'series_8',\n",
    "    'sub-22': 'series_8',\n",
    "    'sub-23': 'series_8',\n",
    "    'sub-24': 'series_8',\n",
    "    'sub-25': 'series_9',\n",
    "    'sub-26': 'series_8',\n",
    "    'sub-27': 'series_10',\n",
    "    'sub-28': 'series_8',\n",
    "    'sub-29': 'series_9',\n",
    "    'sub-30': 'series_8',\n",
    "    'sub-31': 'sub-31',\n",
    "    'sub-32': 'sub-32',\n",
    "    'sub-33': 'series_9',\n",
    "    'sub-34': 'sub-34',\n",
    "    'sub-35': 'series_8',\n",
    "    'sub-36': 'series_8',\n",
    "    'sub-37': 'sub-37',\n",
    "    'sub-38': 'series_10',\n",
    "    'sub-39': 'series_8',\n",
    "    'sub-40': 'series_10',\n",
    "    'sub-41': 'sub-41'}\n",
    "\n",
    "subject_blocks = {'sub-01': 'B','sub-02': 'B','sub-03':'A','sub-04':'A','sub-05':'B','sub-06':'A','sub-07':'B','sub-08':'B',\n",
    "                 'sub-09': 'B', 'sub-10': 'C','sub-11': 'A','sub-12':'B','sub-13':'B','sub-14':'B','sub-15':'C','sub-16':'C',\n",
    "                 'sub-17':'C','sub-18':'C','sub-19':'B','sub-20':'A','sub-21': 'A','sub-22': 'A','sub-23':'A','sub-24':'A','sub-25': 'A',\n",
    "                 'sub-26':'A','sub-27':'A','sub-28':'A','sub-29':'A','sub-30':'A','sub-31':'B','sub-32':'B','sub-33':'A','sub-34':'B',\n",
    "                 'sub-35':'A','sub-36': 'A','sub-37':'B','sub-38': 'A','sub-39': 'A','sub-40': 'A', 'sub-41':'B'} # 'sub-99': 'A'\n",
    "\n",
    "## task settings\n",
    "task_settings = {'task_names_cond': ['discr','easydiff'],\n",
    "                'task_names_full': ['discrA','discrB','discrC','easydiffA','easydiffB','easydiffC','thinkrest'],\n",
    "                'ignore_tasks': ['thinkrest']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e16bcd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-41 : Processed data no longer exists. Participant will be re-processed.\n",
      "Previous conversion detected:  ['sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10', 'sub-11', 'sub-12', 'sub-13', 'sub-14', 'sub-15', 'sub-16', 'sub-17', 'sub-18', 'sub-19', 'sub-20', 'sub-21', 'sub-22', 'sub-23', 'sub-24', 'sub-25', 'sub-26', 'sub-27', 'sub-28', 'sub-29', 'sub-30', 'sub-31', 'sub-32', 'sub-33', 'sub-34', 'sub-35', 'sub-01', 'sub-02', 'sub-36', 'sub-37', 'sub-38', 'sub-39', 'sub-40', 'sub-41'] will not be processed.\n",
      "processing participant: sub-41\n",
      "processing participant: sub-99\n",
      "sub-99: does not have a specific subject run assigned! Check the subject_runs dictionary!\n",
      "****************** CONVERSION COMPLETED ******************\n",
      "**********************************************************\n",
      "**********************************************************\n",
      "Errors and notes regarding the BIDS conversion process will be printed below.\n",
      "sub-41: {'MOCOSERIES_0004'} was not paired with a corresponding MOCOSERIES in the directory or in the run dictionary.\n",
      "sub-99: does not have a specific subject run assigned! Check the subject_runs dictionary!\n"
     ]
    }
   ],
   "source": [
    "progress = {}\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for subject in exclude_subjects:\n",
    "    subs.remove(subject)\n",
    "\n",
    "if auto_detect_progress:\n",
    "    subs,progress = check_progress(subs,progress)\n",
    "\n",
    "process_subjects(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781c41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487cf03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nipype')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ec4350617c6a48f97eef5a74856089ab8d795b18142845719c98ddcfc850b13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
